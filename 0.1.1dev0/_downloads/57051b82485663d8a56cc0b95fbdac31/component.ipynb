{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "!pip install torchx[kfp]\n!wget --no-clobber https://github.com/pytorch/torchx/archive/refs/heads/main.tar.gz\n!tar xf main.tar.gz --strip-components=1\n\nNOTEBOOK = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nTrainer Component Examples\n==========================\n\nComponent definitions that run the example lightning_classy_vision app\nin a single or distributed manner.\n\nPrerequisites of running examples\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nBefore executing examples, install torchx and dependencies necessary to run examples:\n\n.. code:: bash\n\n   pip install torchx\n   git clone https://github.com/pytorch/torchx.git\n   cd torchx\n   pip install -r dev-requirements.txt\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>The working dir should be ``torchx`` to run the components.</p></div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Importing torchx api specifications\n\nfrom typing import Dict, List, Optional\n\nimport torchx\nimport torchx.specs as specs\nfrom torchx.specs import Resource, macros, named_resources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Trainer Component\n###################\nDefines a single trainer component\n\nUse the following cmd to try out:\n\n.. code:: bash\n\n   torchx run --scheduler local_cwd \\\n   ./torchx/examples/apps/lightning_classy_vision/component.py:trainer \\\n   --output_path /tmp\n\nSingle trainer component code:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def trainer(\n    output_path: Optional[str] = None,\n    image: str = torchx.IMAGE,\n    data_path: Optional[str] = None,\n    load_path: str = \"\",\n    log_path: str = \"/tmp/logs\",\n    resource: Optional[str] = None,\n    env: Optional[Dict[str, str]] = None,\n    skip_export: bool = False,\n    epochs: int = 1,\n    layers: Optional[List[int]] = None,\n    learning_rate: Optional[float] = None,\n    num_samples: int = 200,\n) -> specs.AppDef:\n    \"\"\"Runs the example lightning_classy_vision app.\n\n    Args:\n        output_path: output path for model checkpoints (e.g. file:///foo/bar)\n        image: image to run (e.g. foobar:latest)\n        load_path: path to load pretrained model from\n        data_path: path to the data to load, if data_path is not provided,\n            auto generated test data will be used\n        log_path: path to save tensorboard logs to\n        resource: the resources to use\n        env: env variables for the app\n        skip_export: disable model export\n        epochs: number of epochs to run\n        layers: the number of convolutional layers and their number of channels\n        learning_rate: the learning rate to use for training\n        num_samples: the number of images to run per batch, use 0 to run on all\n    \"\"\"\n    env = env or {}\n    args = [\n        \"-m\",\n        \"torchx.examples.apps.lightning_classy_vision.train\",\n        \"--load_path\",\n        load_path,\n        \"--log_path\",\n        log_path,\n        \"--epochs\",\n        str(epochs),\n    ]\n    if output_path:\n        args += [\"--output_path\", output_path]\n    if layers:\n        args += [\"--layers\"] + [str(layer) for layer in layers]\n    if learning_rate:\n        args += [\"--lr\", str(learning_rate)]\n    if num_samples and num_samples > 0:\n        args += [\"--num_samples\", str(num_samples)]\n    if data_path:\n        args += [\"--data_path\", data_path]\n    if skip_export:\n        args.append(\"--skip_export\")\n    return specs.AppDef(\n        name=\"cv-trainer\",\n        roles=[\n            specs.Role(\n                name=\"worker\",\n                entrypoint=\"python\",\n                args=args,\n                env=env,\n                image=image,\n                resource=named_resources[resource]\n                if resource\n                else specs.Resource(cpu=1, gpu=0, memMB=3000),\n            )\n        ],\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Distributed Trainer Component\n###############################\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def trainer_dist(\n    output_path: Optional[str] = None,\n    image: str = torchx.IMAGE,\n    data_path: Optional[str] = None,\n    load_path: str = \"\",\n    log_path: str = \"/tmp/logs\",\n    resource: Optional[str] = None,\n    env: Optional[Dict[str, str]] = None,\n    skip_export: bool = False,\n    epochs: int = 1,\n    nnodes: int = 1,\n    nproc_per_node: int = 1,\n    rdzv_backend: str = \"etcd\",\n    rdzv_endpoint: str = \"etcd-server:2379\",\n) -> specs.AppDef:\n    \"\"\"Runs the example lightning_classy_vision app.\n\n    Args:\n        output_path: output path for model checkpoints (e.g. file:///foo/bar)\n        image: image to run (e.g. foobar:latest)\n        load_path: path to load pretrained model from\n        data_path: path to the data to load, if data_path is not provided,\n            auto generated test data will be used\n        log_path: path to save tensorboard logs to\n        resource: the resources to use\n        env: env variables for the app\n        skip_export: disable model export\n        epochs: number of epochs to run\n        nnodes: number of nodes to run train on, default 1\n        nproc_per_node: number of processes per node. Each process\n            is assumed to use a separate GPU, default 1\n        rdzv_backend: rendezvous backend to use, allowed values can be found at\n            `repistry <https://github.com/pytorch/pytorch/blob/master/torch/distributed/elastic/rendezvous/registry.py>`_\n            The default backend is ``etcd``\n        rdzv_endpoint: Controller endpoint. In case of rdzv_backend is etcd, this is a etcd\n            endpoint, in case of c10d, this is the endpoint of one of the hosts.\n            The default endpoint is ``etcd-server:2379``\n    \"\"\"\n    env = env or {}\n    args = [\n        \"-m\",\n        \"torch.distributed.run\",\n        \"--rdzv_backend\",\n        rdzv_backend,\n        \"--rdzv_endpoint\",\n        rdzv_endpoint,\n        \"--rdzv_id\",\n        f\"{macros.app_id}\",\n        \"--nnodes\",\n        str(nnodes),\n        \"--nproc_per_node\",\n        str(nproc_per_node),\n        \"-m\",\n        \"torchx.examples.apps.lightning_classy_vision.train\",\n        \"--load_path\",\n        load_path,\n        \"--log_path\",\n        log_path,\n        \"--epochs\",\n        str(epochs),\n    ]\n    if output_path:\n        args += [\"--output_path\", output_path]\n    if data_path:\n        args += [\"--data_path\", data_path]\n    if skip_export:\n        args.append(\"--skip_export\")\n    resource_def = (\n        named_resources[resource]\n        if resource\n        else Resource(cpu=nnodes, gpu=0, memMB=3000)\n    )\n    return specs.AppDef(\n        name=\"cv-trainer\",\n        roles=[\n            specs.Role(\n                name=\"worker\",\n                entrypoint=\"python\",\n                args=args,\n                env=env,\n                image=image,\n                resource=resource_def,\n                num_replicas=nnodes,\n            )\n        ],\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Interpreting the Model\n#######################\nDefines a component that interprets the model\n\nTrain a single trainer example: `examples_apps/lightning_classy_vision/component:Trainer Component`\nAnd use the following cmd to try out:\n\n.. code:: bash\n\n   torchx run --scheduler local_cwd \\\n   ./torchx/examples/apps/lightning_classy_vision/component.py:interpret \\\n   --output_path /tmp/aivanou/interpret  --load_path /tmp/$USER/last.ckpt\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def interpret(\n    load_path: str,\n    output_path: str,\n    data_path: Optional[str] = None,\n    image: str = torchx.IMAGE,\n    resource: Optional[str] = None,\n) -> specs.AppDef:\n    \"\"\"Runs the model interpretability app on the model outputted by the training\n    component.\n\n    Args:\n        load_path: path to load pretrained model from\n        output_path: output path for model checkpoints (e.g. file:///foo/bar)\n        data_path: path to the data to load\n        image: image to run (e.g. foobar:latest)\n        resource: the resources to use\n    \"\"\"\n    args = [\n        \"-m\",\n        \"torchx.examples.apps.lightning_classy_vision.interpret\",\n        \"--load_path\",\n        load_path,\n        \"--output_path\",\n        output_path,\n    ]\n    if data_path:\n        args += [\n            \"--data_path\",\n            data_path,\n        ]\n\n    return specs.AppDef(\n        name=\"cv-interpret\",\n        roles=[\n            specs.Role(\n                name=\"worker\",\n                entrypoint=\"python\",\n                args=args,\n                image=image,\n                resource=named_resources[resource]\n                if resource\n                else specs.Resource(cpu=1, gpu=0, memMB=1024),\n            )\n        ],\n    )"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}