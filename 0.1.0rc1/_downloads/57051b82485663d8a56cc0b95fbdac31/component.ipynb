{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "!pip install torchx[kfp]\n!wget --no-clobber https://github.com/pytorch/torchx/archive/refs/heads/main.tar.gz\n!tar xf main.tar.gz --strip-components=1\n\nNOTEBOOK = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nTrainer Component Examples\n==========================\n\nComponent definitions that run the example lightning_classy_vision app\nin a single or distributed manner.\n\nBefore executing examples, install torchx and dependencies necessary to run examples:\n\n.. code:: bash\n\n   pip install torchx\n   git clone https://github.com/pytorch/torchx.git\n   cd torchx\n   pip install -r dev-requirements.txt\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>The working dir should be `torchx` to run the components.</p></div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Importing torchx api specifications\n\nfrom typing import Optional, Dict, List\n\nimport torchx.specs.api as torchx\nfrom torchx.specs import macros, named_resources, Resource\nfrom torchx.version import TORCHX_IMAGE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Single Trainer Component\n#########################\nDefines a single trainer component\n\nUse the following cmd to try out:\n\n.. code:: bash\n\n   torchx run --scheduler local_cwd \\\n   ./torchx/examples/apps/lightning_classy_vision/component.py:trainer \\\n   --output_path /tmp\n\nSingle trainer component code:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def trainer(\n    output_path: str,\n    image: str = TORCHX_IMAGE,\n    data_path: Optional[str] = None,\n    load_path: str = \"\",\n    log_path: str = \"/tmp/logs\",\n    resource: Optional[str] = None,\n    env: Optional[Dict[str, str]] = None,\n    skip_export: bool = False,\n    epochs: int = 1,\n    layers: Optional[List[int]] = None,\n    learning_rate: Optional[float] = None,\n    num_samples: int = 200,\n) -> torchx.AppDef:\n    \"\"\"Runs the example lightning_classy_vision app.\n\n    Args:\n        output_path: output path for model checkpoints (e.g. file:///foo/bar)\n        image: image to run (e.g. foobar:latest)\n        load_path: path to load pretrained model from\n        data_path: path to the data to load, if data_path is not provided,\n            auto generated test data will be used\n        log_path: path to save tensorboard logs to\n        resource: the resources to use\n        env: env variables for the app\n        skip_export: disable model export\n        epochs: number of epochs to run\n        layers: the number of convolutional layers and their number of channels\n        learning_rate: the learning rate to use for training\n        num_samples: the number of images to run per batch, use 0 to run on all\n    \"\"\"\n    env = env or {}\n    args = [\n        \"-m\",\n        \"torchx.examples.apps.lightning_classy_vision.train\",\n        \"--output_path\",\n        output_path,\n        \"--load_path\",\n        load_path,\n        \"--log_path\",\n        log_path,\n        \"--epochs\",\n        str(epochs),\n    ]\n    if layers:\n        args += [\"--layers\"] + [str(layer) for layer in layers]\n    if learning_rate:\n        args += [\"--lr\", str(learning_rate)]\n    if num_samples and num_samples > 0:\n        args += [\"--num_samples\", str(num_samples)]\n    if data_path:\n        args += [\"--data_path\", data_path]\n    if skip_export:\n        args.append(\"--skip_export\")\n    return torchx.AppDef(\n        name=\"cv-trainer\",\n        roles=[\n            torchx.Role(\n                name=\"worker\",\n                entrypoint=\"python\",\n                args=args,\n                env=env,\n                image=image,\n                resource=named_resources[resource]\n                if resource\n                else torchx.Resource(cpu=1, gpu=0, memMB=3000),\n            )\n        ],\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Distributed Trainer Component\n##############################\nDefines distributed trainer component\n\nUse the following cmd to try out:\n\n.. code:: bash\n\n      torchx run --scheduler local_cwd \\\n      ./torchx/examples/apps/lightning_classy_vision/component.py:trainer_dist \\\n      --output_path /tmp --rdzv_backend c10d --rdzv_endpoint localhost:29500\n\n\nExecuting distributed trainer job\n#################################\n\nTorchX supports kubernetes scheduler that allows users to execute distributed job in kubernetes cluster.\nIt uses `Volcano <https://volcano.sh/en/>`_ to schedule jobs.\n\nDistributed trainer uses `torch.distributed.run <https://pytorch.org/docs/stable/elastic/quickstart.html>`_\nto start user processes. There are two rendezvous types that can be used to execute\ndistributed jobs: `c10d` and `etcd`.\n\nPrerequisites to execute distributed jobs on kubernetes cluster:\n\n* Install volcano 1.4.0 version\n\n.. code:: bash\n\n      kubectl apply -f https://raw.githubusercontent.com/volcano-sh/volcano/v1.4.0/installer/volcano-development.yaml\n\n* Install etcd server on your kubernetes cluster:\n\n      kubectl apply -f https://github.com/pytorch/torchx/blob/main/resources/etcd.yaml\n\nAfter that the job can be executed on kubernetes:\n\n.. code:: bash\n\n torchx run --scheduler kubernetes --scheduler_args namespace=default,queue=default \\\n ./torchx/examples/apps/lightning_classy_vision/component.py:trainer_dist \\\n --nnodes 2 --epochs 1 --output_path /tmp\n\nDistributed trainer component code:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def trainer_dist(\n    output_path: str,\n    image: str = TORCHX_IMAGE,\n    data_path: Optional[str] = None,\n    load_path: str = \"\",\n    log_path: str = \"/tmp/logs\",\n    resource: Optional[str] = None,\n    env: Optional[Dict[str, str]] = None,\n    skip_export: bool = False,\n    epochs: int = 1,\n    nnodes: int = 1,\n    nproc_per_node: int = 1,\n    rdzv_backend: str = \"etcd\",\n    rdzv_endpoint: str = \"etcd-server:2379\",\n) -> torchx.AppDef:\n    \"\"\"Runs the example lightning_classy_vision app.\n\n    Args:\n        output_path: output path for model checkpoints (e.g. file:///foo/bar)\n        image: image to run (e.g. foobar:latest)\n        load_path: path to load pretrained model from\n        data_path: path to the data to load, if data_path is not provided,\n            auto generated test data will be used\n        log_path: path to save tensorboard logs to\n        resource: the resources to use\n        env: env variables for the app\n        skip_export: disable model export\n        epochs: number of epochs to run\n        nnodes: number of nodes to run train on, default 1\n        nproc_per_node: number of processes per node. Each process\n            is assumed to use a separate GPU, default 1\n        rdzv_backend: rendezvous backend to use, allowed values can be found at\n            `repistry <https://github.com/pytorch/pytorch/blob/master/torch/distributed/elastic/rendezvous/registry.py>`_\n            The default backend is ``etcd``\n        rdzv_endpoint: Controller endpoint. In case of rdzv_backend is etcd, this is a etcd\n            endpoint, in case of c10d, this is the endpoint of one of the hosts.\n            The default endpoint is ``etcd-server:2379``\n    \"\"\"\n    env = env or {}\n    args = [\n        \"-m\",\n        \"torch.distributed.run\",\n        \"--rdzv_backend\",\n        rdzv_backend,\n        \"--rdzv_endpoint\",\n        rdzv_endpoint,\n        \"--rdzv_id\",\n        f\"{macros.app_id}\",\n        \"--nnodes\",\n        str(nnodes),\n        \"--nproc_per_node\",\n        str(nproc_per_node),\n        \"-m\",\n        \"torchx.examples.apps.lightning_classy_vision.train\",\n        \"--output_path\",\n        output_path,\n        \"--load_path\",\n        load_path,\n        \"--log_path\",\n        log_path,\n        \"--epochs\",\n        str(epochs),\n    ]\n    if data_path:\n        args += [\"--data_path\", data_path]\n    if skip_export:\n        args.append(\"--skip_export\")\n    resource_def = (\n        named_resources[resource]\n        if resource\n        else Resource(cpu=nnodes, gpu=0, memMB=3000)\n    )\n    return torchx.AppDef(\n        name=\"cv-trainer\",\n        roles=[\n            torchx.Role(\n                name=\"worker\",\n                entrypoint=\"python\",\n                args=args,\n                env=env,\n                image=image,\n                resource=resource_def,\n                num_replicas=nnodes,\n            )\n        ],\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Model Interpretability\n#######################\nTODO(aivanou): add documentation\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def interpret(\n    image: str,\n    load_path: str,\n    data_path: str,\n    output_path: str,\n    resource: Optional[str] = None,\n) -> torchx.AppDef:\n    \"\"\"Runs the model interpretability app on the model outputted by the training\n    component.\n\n    Args:\n        image: image to run (e.g. foobar:latest)\n        load_path: path to load pretrained model from\n        data_path: path to the data to load\n        output_path: output path for model checkpoints (e.g. file:///foo/bar)\n        resource: the resources to use\n    \"\"\"\n    return torchx.AppDef(\n        name=\"cv-interpret\",\n        roles=[\n            torchx.Role(\n                name=\"worker\",\n                entrypoint=\"python\",\n                args=[\n                    \"-m\",\n                    \"torchx.examples.apps.lightning_classy_vision.interpret\",\n                    \"--load_path\",\n                    load_path,\n                    \"--data_path\",\n                    data_path,\n                    \"--output_path\",\n                    output_path,\n                ],\n                image=image,\n                resource=named_resources[resource]\n                if resource\n                else torchx.Resource(cpu=1, gpu=0, memMB=1024),\n            )\n        ],\n    )"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}