{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "!pip install torchx[kfp]\n!wget --no-clobber https://github.com/pytorch/torchx/archive/refs/heads/main.tar.gz\n!tar xf main.tar.gz --strip-components=1\n\nNOTEBOOK = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nAdvanced KubeFlow Pipelines Example\n===================================\n\nThis is an example pipeline using KubeFlow Pipelines built with only TorchX\ncomponents.\n\nKFP adapters can be used transform the TorchX components directly into\nsomething that can be used within KFP.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Input Arguments\n###############\nLets first define some arguments for the pipeline.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import argparse\nimport os.path\nimport sys\nfrom typing import Dict\n\nimport kfp\nimport torchx\nfrom torchx import specs\nfrom torchx.components.dist import ddp as dist_ddp\nfrom torchx.components.serve import torchserve\nfrom torchx.components.utils import copy as utils_copy, python as utils_python\nfrom torchx.pipelines.kfp.adapter import container_from_app\n\n\nparser = argparse.ArgumentParser(description=\"example kfp pipeline\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TorchX components are built around images. Depending on what scheduler\nyou're using this can vary but for KFP these images are specified as\ndocker containers. We have one container for the example apps and one for\nthe standard built in apps. If you modify the torchx example code you'll\nneed to rebuild the container before launching it on KFP\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "parser.add_argument(\n    \"--image\",\n    type=str,\n    help=\"docker image to use for the examples apps\",\n    default=torchx.IMAGE,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Most TorchX components use\n`fsspec <https://filesystem-spec.readthedocs.io/en/latest/>`_ to abstract\naway dealing with remote filesystems. This allows the components to take\npaths like ``s3://`` to make it easy to use cloud storage providers.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "parser.add_argument(\n    \"--output_path\",\n    type=str,\n    help=\"path to place the data\",\n    required=True,\n)\nparser.add_argument(\"--load_path\", type=str, help=\"checkpoint path to load from\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This example uses the torchserve for inference so we need to specify some\noptions. This assumes you have a TorchServe instance running in the same\nKubernetes cluster with with the service name ``torchserve`` in the default\nnamespace.\n\nSee https://github.com/pytorch/serve/blob/master/kubernetes/README.md for info\non how to setup TorchServe.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "parser.add_argument(\n    \"--management_api\",\n    type=str,\n    help=\"path to the torchserve management API\",\n    default=\"http://torchserve.default.svc.cluster.local:8081\",\n)\nparser.add_argument(\n    \"--model_name\",\n    type=str,\n    help=\"the name of the inference model\",\n    default=\"tiny_image_net\",\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "notebook.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if \"NOTEBOOK\" in globals():\n    argv = [\n        \"--output_path\",\n        \"/tmp/output\",\n    ]\nelse:\n    argv = sys.argv[1:]\n\nargs: argparse.Namespace = parser.parse_args(argv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creating the Components\n#######################\nThe first step is downloading the data to somewhere we can work on it. For\nthis we can just the builtin copy component. This component takes two valid\nfsspec paths and copies them from one to another. In this case we're using\nhttp as the source and a file under the output_path as the output.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data_path: str = os.path.join(args.output_path, \"tiny-imagenet-200.zip\")\ncopy_app: specs.AppDef = utils_copy(\n    \"http://cs231n.stanford.edu/tiny-imagenet-200.zip\",\n    data_path,\n    image=args.image,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The next component is for data preprocessing. This takes in the raw data from\nthe previous operator and runs some transforms on it for use with the trainer.\n\ndatapreproc outputs the data to a specified fsspec path. These paths are all\nspecified ahead of time so we have a fully static pipeline.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "processed_data_path: str = os.path.join(args.output_path, \"processed\")\ndatapreproc_app: specs.AppDef = utils_python(\n    \"--output_path\",\n    processed_data_path,\n    \"--input_path\",\n    data_path,\n    \"--limit\",\n    \"100\",\n    image=args.image,\n    m=\"torchx.examples.apps.datapreproc.datapreproc\",\n    cpu=1,\n    memMB=1024,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next we'll create the trainer component that takes in the training data from the\nprevious datapreproc component. We've defined this in a separate component\nfile as you normally would.\n\nHaving a separate component file allows you to launch your trainer from the\nTorchX CLI via ``torchx run`` for fast iteration as well as run it from a\npipeline in an automated fashion.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# make sure examples is on the path\nif \"__file__\" in globals():\n    sys.path.append(os.path.join(os.path.dirname(__file__), \"..\", \"..\", \"..\"))\n\n\nlogs_path: str = os.path.join(args.output_path, \"logs\")\nmodels_path: str = os.path.join(args.output_path, \"models\")\n\ntrainer_app: specs.AppDef = dist_ddp(\n    *(\n        \"--output_path\",\n        models_path,\n        \"--load_path\",\n        args.load_path or \"\",\n        \"--log_path\",\n        logs_path,\n        \"--data_path\",\n        processed_data_path,\n        \"--epochs\",\n        str(1),\n    ),\n    image=args.image,\n    m=\"torchx.examples.apps.lightning.train\",\n    j=\"1x1\",\n    # per node resource settings\n    cpu=1,\n    memMB=3000,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To have the tensorboard path show up in KFPs UI we need to some metadata so\nKFP knows where to consume the metrics from.\n\nThis will get used when we create the KFP container.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ui_metadata: Dict[str, object] = {\n    \"outputs\": [\n        {\n            \"type\": \"tensorboard\",\n            \"source\": os.path.join(logs_path, \"lightning_logs\"),\n        }\n    ]\n}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the inference, we're leveraging one of the builtin TorchX components. This\ncomponent takes in a model and uploads it to the TorchServe management API\nendpoints.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "serve_app: specs.AppDef = torchserve(\n    model_path=os.path.join(models_path, \"model.mar\"),\n    management_api=args.management_api,\n    image=args.image,\n    params={\n        \"model_name\": args.model_name,\n        # set this to allocate a worker\n        # \"initial_workers\": 1,\n    },\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For model interpretability we're leveraging a custom component stored in it's\nown component file. This component takes in the output from datapreproc and\ntrain components and produces images with integrated gradient results.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "interpret_path: str = os.path.join(args.output_path, \"interpret\")\ninterpret_app: specs.AppDef = utils_python(\n    *(\n        \"--load_path\",\n        os.path.join(models_path, \"last.ckpt\"),\n        \"--data_path\",\n        processed_data_path,\n        \"--output_path\",\n        interpret_path,\n    ),\n    image=args.image,\n    m=\"torchx.examples.apps.lightning.interpret\",\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pipeline Definition\n###################\nThe last step is to define the actual pipeline using the torchx components via\nthe KFP adapter and export the pipeline package that can be uploaded to a KFP\ncluster.\n\nThe KFP adapter currently doesn't track the input and outputs so the\ncontainers need to have their dependencies specified via `.after()`.\n\nWe call `.set_tty()` to make the logs from the components more responsive for\nexample purposes.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def pipeline() -> None:\n    # container_from_app creates a KFP container from the TorchX app\n    # definition.\n    copy = container_from_app(copy_app)\n    copy.container.set_tty()\n\n    datapreproc = container_from_app(datapreproc_app)\n    datapreproc.container.set_tty()\n    datapreproc.after(copy)\n\n    # For the trainer we want to log that UI metadata so you can access\n    # tensorboard from the UI.\n    trainer = container_from_app(trainer_app, ui_metadata=ui_metadata)\n    trainer.container.set_tty()\n    trainer.after(datapreproc)\n\n    if False:\n        serve = container_from_app(serve_app)\n        serve.container.set_tty()\n        serve.after(trainer)\n\n    if False:\n        # Serve and interpret only require the trained model so we can run them\n        # in parallel to each other.\n        interpret = container_from_app(interpret_app)\n        interpret.container.set_tty()\n        interpret.after(trainer)\n\n\nkfp.compiler.Compiler().compile(\n    pipeline_func=pipeline,\n    package_path=\"pipeline.yaml\",\n)\n\nwith open(\"pipeline.yaml\", \"rt\") as f:\n    print(f.read())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once this has all run you should have a pipeline file (typically\npipeline.yaml) that you can upload to your KFP cluster via the UI or\na kfp.Client.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# sphinx_gallery_thumbnail_path = '_static/img/gallery-kfp.png'"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}