{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff219421",
   "metadata": {},
   "source": [
    "# Airflow\n",
    "\n",
    "For pipelines that support Python based execution you can directly use the\n",
    "TorchX API. TorchX is designed to be easily integrated in to other applications\n",
    "via the programmatic API. No special Airflow integrations are needed.\n",
    "\n",
    "With TorchX, you can use Airflow for the pipeline orchestration and run your\n",
    "PyTorch application (i.e. distributed training) on a remote GPU cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e536456",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T21:53:38.149453Z",
     "iopub.status.busy": "2025-05-05T21:53:38.149258Z",
     "iopub.status.idle": "2025-05-05T21:53:38.868657Z",
     "shell.execute_reply": "2025-05-05T21:53:38.867903Z"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pendulum\n",
    "\n",
    "from airflow.utils.state import DagRunState, TaskInstanceState\n",
    "from airflow.utils.types import DagRunType\n",
    "from airflow.models.dag import DAG\n",
    "from airflow.decorators import task\n",
    "\n",
    "\n",
    "DATA_INTERVAL_START = pendulum.datetime(2021, 9, 13, tz=\"UTC\")\n",
    "DATA_INTERVAL_END = DATA_INTERVAL_START + datetime.timedelta(days=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc00f0b",
   "metadata": {},
   "source": [
    "To launch a TorchX job from Airflow you can create a Airflow Python task to\n",
    "import the runner, launch the job and wait for it to complete. If you're running\n",
    "on a remote cluster you may need to use the virtualenv task to install the\n",
    "`torchx` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cdb2ef6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T21:53:38.871464Z",
     "iopub.status.busy": "2025-05-05T21:53:38.870966Z",
     "iopub.status.idle": "2025-05-05T21:53:38.875918Z",
     "shell.execute_reply": "2025-05-05T21:53:38.875331Z"
    }
   },
   "outputs": [],
   "source": [
    "@task(task_id=f'hello_torchx')\n",
    "def run_torchx(message):\n",
    "    \"\"\"This is a function that will run within the DAG execution\"\"\"\n",
    "    from torchx.runner import get_runner\n",
    "    with get_runner() as runner:\n",
    "        # Run the utils.sh component on the local_cwd scheduler.\n",
    "        app_id = runner.run_component(\n",
    "            \"utils.sh\",\n",
    "            [\"echo\", message],\n",
    "            scheduler=\"local_cwd\",\n",
    "        )\n",
    "\n",
    "        # Wait for the the job to complete\n",
    "        status = runner.wait(app_id, wait_interval=1)\n",
    "\n",
    "        # Raise_for_status will raise an exception if the job didn't succeed\n",
    "        status.raise_for_status()\n",
    "\n",
    "        # Finally we can print all of the log lines from the TorchX job so it\n",
    "        # will show up in the workflow logs.\n",
    "        for line in runner.log_lines(app_id, \"sh\", k=0):\n",
    "            print(line, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bb3446",
   "metadata": {},
   "source": [
    "Once we have the task defined we can put it into a Airflow DAG and run it like\n",
    "normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c211bf7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T21:53:38.877820Z",
     "iopub.status.busy": "2025-05-05T21:53:38.877598Z",
     "iopub.status.idle": "2025-05-05T21:53:41.520972Z",
     "shell.execute_reply": "2025-05-05T21:53:41.520301Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">/tmp/ipykernel_4911/454499020.py:</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">3</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> RemovedInAirflow3Warning</span><span style=\"color: #808000; text-decoration-color: #808000\">: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33m/tmp/ipykernel_4911/\u001b[0m\u001b[1;33m454499020.py\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m3\u001b[0m\u001b[1;33m RemovedInAirflow3Warning\u001b[0m\u001b[33m: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[34m2025-05-05T21:53:39.116+0000\u001b[0m] {\u001b[34mtaskinstance.py:\u001b[0m2614} INFO\u001b[0m - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: example_python_operator-pfwpkvq53kb9s.hello_torchx manual__2021-09-13T00:00:00+00:00 [None]>\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[34m2025-05-05T21:53:39.123+0000\u001b[0m] {\u001b[34mtaskinstance.py:\u001b[0m2614} INFO\u001b[0m - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: example_python_operator-pfwpkvq53kb9s.hello_torchx manual__2021-09-13T00:00:00+00:00 [None]>\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[34m2025-05-05T21:53:39.124+0000\u001b[0m] {\u001b[34mtaskinstance.py:\u001b[0m2867} INFO\u001b[0m - Starting attempt 0 of 1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[34m2025-05-05T21:53:39.125+0000\u001b[0m] {\u001b[34mtaskinstance.py:\u001b[0m2948} \u001b[33mWARNING\u001b[0m - \u001b[33mcannot record queued_duration for task hello_torchx because previous state change time has not been saved\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[34m2025-05-05T21:53:39.136+0000\u001b[0m] {\u001b[34mtaskinstance.py:\u001b[0m2890} INFO\u001b[0m - Executing <Task(_PythonDecoratedOperator): hello_torchx> on 2021-09-13 00:00:00+00:00\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[34m2025-05-05T21:53:39.710+0000\u001b[0m] {\u001b[34mtaskinstance.py:\u001b[0m3134} INFO\u001b[0m - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='example_python_operator-pfwpkvq53kb9s' AIRFLOW_CTX_TASK_ID='hello_torchx' AIRFLOW_CTX_EXECUTION_DATE='2021-09-13T00:00:00+00:00' AIRFLOW_CTX_DAG_RUN_ID='manual__2021-09-13T00:00:00+00:00'\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task instance is in running state\n",
      " Previous state of the Task instance: queued\n",
      "Current task name:hello_torchx state:running start_date:2025-05-05 21:53:39.118703+00:00\n",
      "Dag name:example_python_operator-pfwpkvq53kb9s and current dag run status:running\n",
      "[\u001b[34m2025-05-05T21:53:39.715+0000\u001b[0m] {\u001b[34mtaskinstance.py:\u001b[0m732} INFO\u001b[0m - ::endgroup::\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[34m2025-05-05T21:53:40.393+0000\u001b[0m] {\u001b[34mapi.py:\u001b[0m75} INFO\u001b[0m - Tracker configurations: {}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[34m2025-05-05T21:53:40.397+0000\u001b[0m] {\u001b[34mlocal_scheduler.py:\u001b[0m787} INFO\u001b[0m - Log directory not set in scheduler cfg. Creating a temporary log dir that will be deleted on exit. To preserve log directory set the `log_dir` cfg option\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[34m2025-05-05T21:53:40.398+0000\u001b[0m] {\u001b[34mlocal_scheduler.py:\u001b[0m793} INFO\u001b[0m - Log directory is: /tmp/torchx_lj3szbl3\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, TorchX!\n",
      "[\u001b[34m2025-05-05T21:53:41.504+0000\u001b[0m] {\u001b[34mpython.py:\u001b[0m240} INFO\u001b[0m - Done. Returned value was: None\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[34m2025-05-05T21:53:41.508+0000\u001b[0m] {\u001b[34mtaskinstance.py:\u001b[0m341} INFO\u001b[0m - ::group::Post task execution logs\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[34m2025-05-05T21:53:41.508+0000\u001b[0m] {\u001b[34mtaskinstance.py:\u001b[0m353} INFO\u001b[0m - Marking task as SUCCESS. dag_id=example_python_operator-pfwpkvq53kb9s, task_id=hello_torchx, run_id=manual__2021-09-13T00:00:00+00:00, execution_date=20210913T000000, start_date=20250505T215339, end_date=20250505T215341\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task instance in success state\n",
      " Previous state of the Task instance: running\n",
      "Dag name:example_python_operator-pfwpkvq53kb9s queued_at:None\n",
      "Task hostname:runner.rccwnyb1mo5uxnpxrcrpztboae.gx.internal.cloudapp.net operator:_PythonDecoratedOperator\n"
     ]
    }
   ],
   "source": [
    "from torchx.schedulers.ids import make_unique\n",
    "\n",
    "with DAG(\n",
    "    dag_id=make_unique('example_python_operator'),\n",
    "    schedule_interval=None,\n",
    "    start_date=DATA_INTERVAL_START,\n",
    "    catchup=False,\n",
    "    tags=['example'],\n",
    ") as dag:\n",
    "    run_job = run_torchx(\"Hello, TorchX!\")\n",
    "\n",
    "\n",
    "dagrun = dag.create_dagrun(\n",
    "    state=DagRunState.RUNNING,\n",
    "    execution_date=DATA_INTERVAL_START,\n",
    "    data_interval=(DATA_INTERVAL_START, DATA_INTERVAL_END),\n",
    "    start_date=DATA_INTERVAL_END,\n",
    "    run_type=DagRunType.MANUAL,\n",
    ")\n",
    "ti = dagrun.get_task_instance(task_id=\"hello_torchx\")\n",
    "ti.task = dag.get_task(task_id=\"hello_torchx\")\n",
    "ti.run(ignore_ti_state=True)\n",
    "assert ti.state == TaskInstanceState.SUCCESS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8db61ba",
   "metadata": {},
   "source": [
    "If all goes well you should see `Hello, TorchX!` printed above.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "* Checkout the [runner API documentation](../runner.rst) to learn more about\n",
    "  programmatic usage of TorchX\n",
    "* Browse through the collection of [builtin components](../components/overview.rst)\n",
    "  which can be used in your Airflow pipeline"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "markdown",
    "format_version": "1.3",
    "jupytext_version": "1.13.7"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
